# -*- coding: utf-8 -*-
"""Dheeraj_Dev CD_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1trQnematS7Hp8ZFaR98XFDkJ4gtuvGnz
"""

import pandas as pd 
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
from sklearn.cluster import MeanShift
from sklearn import metrics
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import normalize

"""Reading data set"""

df = pd.read_csv("data.csv", delimiter=",", names=["c_1", "c_2"])
df

df.describe()

plt.scatter(df["c_1"], df["c_2"], s=5)

plt.scatter(df["c_1"], df["c_2"], s=25)

#pairwise relationships in the dataset
import seaborn as sns
a = sns.PairGrid(df)
a.map(sns.scatterplot)

scores = []
values = np.arange(2, 11)

for num_clusters in values:
  kmeans = KMeans(n_clusters = num_clusters, 
                  init = 'k-means++', 
                  max_iter = 500, 
                  n_init = 'auto', 
                  random_state = 0)
  kmeans.fit(df)
  score = metrics.silhouette_score(df, kmeans.labels_, metric = 'euclidean', sample_size = len(df))
  print("\nNumber of clusters =", num_clusters)
  print("Silhouette score = %.3f"%score)
  scores.append(score)

plt.plot(range(2,11),scores)                    # set plot title
plt.xlabel('Number of clusters')                # Set x axis name
plt.ylabel('Silhouette score')                  # Set y axis name
plt.show()

kmeans = KMeans(n_clusters = 2,            # number of clusters
                init = 'k-means++',        # Initialization method for kmeans
                max_iter = 500,            # Maximum number of iterations
                n_init = 'auto',           
                random_state = 4)

y_kmeans = kmeans.fit_predict(df)
y_kmeans

X = np.array(df)
X

X[0][0]

plt.scatter(X[y_kmeans==0, 0],  X[y_kmeans==0,1],s=20, c='purple', label ='Cluster 1')
plt.scatter(X[y_kmeans==1, 0], X[y_kmeans==1, 1], s=2, c='yellow', label ='Cluster 2')

plt.scatter(X[y_kmeans==0, 0],  X[y_kmeans==0,1],s=2, c='purple', label ='Cluster 1')
plt.scatter(X[y_kmeans==1, 0], X[y_kmeans==1, 1], s=20, c='yellow', label ='Cluster 2')

centers = kmeans.cluster_centers_
plt.scatter(df["c_1"], df["c_2"], c=y_kmeans,s=10)
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=50, alpha=0.8)
plt.show()

"""GaussianMixture"""

from sklearn.mixture import GaussianMixture

n_components = np.arange(1,21)
models = [GaussianMixture(n, covariance_type = 'full', random_state = 0).fit(df) for n in n_components]
plt.plot(n_components, [m.bic(df) for m in models], label = 'BIC')
plt.plot(n_components, [m.aic(df) for m in models], label = 'AIC')
plt.legend(loc = 'best')
plt.xlabel('n_components');
plt.show()

gmm = GaussianMixture(n_components = 2)
gmm.fit(df)

labels = gmm.predict(df)
plt.scatter(df["c_1"], df["c_2"], c = labels, cmap = 'viridis', s = 10)
plt.title('GMM')
plt.show()

score = metrics.silhouette_score(df,labels, metric = 'euclidean', sample_size = len(df))
print('Silhoutte score- %.3f'%score)

"""Mean shift"""

standard = StandardScaler() 
standard_scaled = standard.fit_transform(df)
features = df.columns
df_standard = pd.DataFrame(standard_scaled, columns=features)
df_s = df_standard

ms = MeanShift()
ms.fit(df_s)
labels = ms.labels_
cluster_centers = ms.cluster_centers_

print("Center coordinate:\n",cluster_centers)
n_clusters_ = len(np.unique(labels))
print("Estimated clusters:", n_clusters_)

plt.scatter(df["c_1"], df["c_2"], c=labels, s=10)
plt.scatter(cluster_centers[:,0], cluster_centers[:,1], c='black', s=40)
plt.show()

y_ms_pred = ms.predict(df)
score = metrics.silhouette_score(df, ms.labels_, metric = 'euclidean', sample_size = len(df))
print('Silhoutte score- %.3f'%score)

"""Agglomerative clustering"""

import numpy as nm  
import matplotlib.pyplot as mtp  
import pandas as pd

x = df.values

import scipy.cluster.hierarchy as shc  
dendro = shc.dendrogram(shc.linkage(x, method="ward"))  
mtp.title("Dendrogrma Plot")  
mtp.ylabel("Euclidean Distances")  
mtp.xlabel("Customers")  
mtp.show()

from sklearn.cluster import AgglomerativeClustering  
hc= AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')

y_pred= hc.fit_predict(x)

mtp.scatter(x[y_pred == 0, 0], x[y_pred == 0, 1], c = 'purple', label = 'Cluster 1',s = 15)  
mtp.scatter(x[y_pred == 1, 0], x[y_pred == 1, 1], c = 'yellow', label = 'Cluster 2', s=15)   
mtp.title('Clusters of data')  
mtp.xlabel('class1')  
mtp.ylabel('class2')  
mtp.legend()  
mtp.show()

score = metrics.silhouette_score(df, hc.labels_, metric = 'euclidean', sample_size = len(df))
print('Agglomerative Clustering scoreo- %.3f'%score)

"""Spectral clustering"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import SpectralClustering
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

X = df.values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_normalized = normalize(X_scaled)

X_normalized = pd.DataFrame(X_normalized)

pca = PCA(n_components = 2)
X_principal = pca.fit_transform(X_normalized)
X_principal = pd.DataFrame(X_principal)
X_principal.columns = ['P1', 'P2']

X_principal.describe()

from sklearn.cluster import SpectralClustering

colours = {}
colours[0] = 'b'
colours[1] = 'y'
s_c=[]

spectral_model_rbf = SpectralClustering(n_clusters = 2, eigen_solver='arpack', affinity ='rbf',gamma=1, n_neighbors=100,eigen_tol=0.0)

labels_rbf = spectral_model_rbf.fit_predict(X_normalized)

cvec = [colours[label] for label in labels_rbf]

plt.scatter(df["c_1"], df["c_2"], c=labels_rbf, s=8)
plt.title('Spectral clustering')
#plt.legend(('Label 0', 'Label 1'))
plt.show()

print(silhouette_score(X, labels_rbf))

spectral_model_nn = SpectralClustering(n_clusters = 2, affinity ='nearest_neighbors',gamma=1.0,n_neighbors=100,eigen_tol=0.0)

labels_nn = spectral_model_nn.fit_predict(X_normalized)

cvec = [colours[label] for label in labels_nn]

print(silhouette_score(X, labels_nn))

"""DBscan"""

import sys
np.set_printoptions(threshold = sys.maxsize)
from sklearn.cluster import DBSCAN

X=df.values

y_pred = DBSCAN(eps = 0.2, min_samples = 100).fit(X)
labels = y_pred.labels_
core_samples_mask = np.zeros_like(y_pred.labels_, dtype=bool)
core_samples_mask[y_pred.core_sample_indices_] = True

n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
print('Number of clusters', n_clusters_)

no_clusters = len(np.unique(labels))
no_noise = np.sum(np.array(labels) == -1, axis=0)

from sklearn import cluster
dbscan = cluster.DBSCAN(eps=0.2, min_samples=100)
clustering_labels = dbscan.fit_predict(df)

#y_p=DBSCAN(eps = 0.3, min_samples = 300)
#labels_db = DBSCAN(eps = 0.3, min_samples = 300).fit_predict(df)

plt.scatter(df["c_1"], df["c_2"], c=clustering_labels, s=8)
plt.title('DBscan clustering')
#plt.legend(('Label 0', 'Label 1'))
plt.show()

unique_labels = set(labels)
colors = ['y', 'b', 'g', 'r']

for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        # col = 'k'
        continue
  
    class_member_mask = (labels == k)
  
    xy = X[class_member_mask & core_samples_mask]
    
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,
             markeredgecolor='k',
             markersize=6)
  
    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,
             markeredgecolor='k',
             markersize=6)
y_predicted = np.array([i for i in labels if i != -1])
plt.title('number of clusters: %d' % n_clusters_)
plt.show()

df['labels'] = clustering_labels

score = metrics.silhouette_score(df, df['labels'])
print(score)

clustering = cluster.DBSCAN(eps=0.2, min_samples=100)

y_pred = clustering.fit_predict(df)
y_pred

new_arr=[]

import random
for i in y_pred:
  if(i==-1):
    new_arr.append(random.randint(0,1))
  else:
    new_arr.append(i)

new_arr

# exporting output
file = open("Dheeraj_DevCD_testdataclasslabels.txt", "w")
np.savetxt(file, new_arr, fmt='%d')
    
file.close()

